---
############################################
######### Kafka topics & schemas ###########
- name: "Delete Kafka topics"
  include: toolbox_exec.yml
  vars:
    toolbox_ignore_err: true
    toolbox_debug: "{{ debug_mode }}"
    bash_one_liner: >-
      export JAVA_HOME=/usr/lib/jvm/zulu-8-amd64;
      kafka-topics
      --zookeeper {{ groups['cp_zookeeper_nodes'][0] }}:2181
      --delete
      --topic {{ item.name }}
  loop: "{{ topics | flatten(levels=1) }}"
  when: pristine_system or absent_from_system

- name: "Create Kafka topics"
  include: toolbox_exec.yml
  vars:
    toolbox_ignore_err: false
    toolbox_debug: "{{ debug_mode }}"
    bash_one_liner: >-
      export JAVA_HOME=/usr/lib/jvm/zulu-8-amd64;
      kafka-topics
      --zookeeper {{ groups['cp_zookeeper_nodes'][0] }}:2181
      --create
      --if-not-exists
      --topic {{ item.name }}
      --partitions {{ item.partitions }}
      --replication-factor {{ item.replication_factor }}

      kafka-topics
      --zookeeper {{ groups['cp_zookeeper_nodes'][0] }}:2181
      --alter
      --topic {{ item.name }}
      {% for config in lookup('dict', item.config)  %}
      --config {{ config.key }}={{ config.value }}
      {% endfor %}
  loop: "{{ topics | flatten(levels=1) }}"
  when: not absent_from_system

- name: "Delete Kafka topics schema"
  uri:
    url: "http://{{ groups['cp_schema_registry'][0] }}:18081/subjects/{{ item.name }}-value"
    method: DELETE
  loop: "{{ topics | flatten(levels=1) }}"
  ignore_errors: true
  when: pristine_system or absent_from_system

- name: "Publish Kafka topics schema"
  uri:
    url: "http://{{ groups['cp_schema_registry'][0] }}:18081/subjects/{{ item.name }}-value/versions"
    headers:
      Content-Type: "application/vnd.schemaregistry.v1+json"
    method: POST
    body_format: json
    body: >-
      {"schema": "{{ lookup("file",  avro_schema_folder + "/" + item.avro.value_schema_file) |  regex_replace('\n| |\t', '') | regex_replace('"', '\"') }}"}
  loop: "{{ topics | flatten(levels=1) }}"
  when: not absent_from_system

- meta: end_play
#
#
#############################################
########### Flink session & job #############
## https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/yarn_setup.html
## TODO: check what happens when run twice...may need to stop session first ?
##     => echo "stop" | ./bin/yarn-session.sh -id "flink-session-daemon"
#- name: "Start Flink detached session"
#  include: toolbox_exec.yml
#  vars:
#    bash_one_liner: >-
#      ${FLINK_FOLDER}/bin/yarn-session.sh
#        -nm "flink-session-daemon"
#        -jm {{ job_manager.memory }}
#        -n  {{ task_manager.node_count }}
#        -s  {{ task_manager.slots }}
#        -tm {{ task_manager.memory }}
#        -st -d
#
#- name: "Copy Flink job artifact to panda-toolbox"
#  become: yes
#  copy:
#    src: "{{ role_path }}/../../digitalpanda-bigdata/sensor-digestion-flink/target/scala-2.11/sensor-digestion-flink-assembly-0.1-SNAPSHOT.jar"
#    dest: "{{ toolbox.host.folders.state }}/sensor-digestion-flink-assembly-0.1-SNAPSHOT.jar"
#    owner: "{{ cluster_config_user }}"
#    group: "{{ cluster_config_user }}"
#    mode: "u=rx,g=rx,o=rx"
#
## TODO: Stop job if already started
#
#- name: "Upload & start Flink job"
#  include: toolbox_exec.yml
#  vars:
#    bash_one_liner: >-
#      ${FLINK_FOLDER}/bin/flink
#        run ${TOOLBOX_STATE}/sensor-digestion-flink-assembly-0.1-SNAPSHOT.jar
#
#
#############################################
########### Kafka connect DB sinks ##########
#
#- name: "Create Cassandra tables"
#  include: cassandra_manage.yml # use toolbox instead....
#  vars:
#    command_args: ./cqlsh -e "lookup('template', 'hosts')"
#
#- name: "Create Kafka connectors"
#
#- name: "Start Kafka connectors"
