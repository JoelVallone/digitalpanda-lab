---
############################################
######### Kafka topics & schemas ###########
- name: "Delete Kafka topics"
  include: toolbox_exec.yml
  vars:
    toolbox_ignore_err: true
    toolbox_debug: "{{ debug_mode }}"
    bash_one_liner: >-
      export JAVA_HOME=/usr/lib/jvm/zulu-8-amd64;
      kafka-topics
      --zookeeper {{ groups['cp_zookeeper_nodes'][0] }}:2181
      --delete
      --topic {{ item.name }}
  loop: "{{ topics | flatten(levels=1) }}"
  when:  pristine_system or absent_from_system

- name: "Create Kafka topics"
  include: toolbox_exec.yml
  vars:
    toolbox_ignore_err: false
    toolbox_debug: "{{ debug_mode }}"
    bash_one_liner: >-
      export JAVA_HOME=/usr/lib/jvm/zulu-8-amd64;
      kafka-topics
      --zookeeper {{ groups['cp_zookeeper_nodes'][0] }}:2181
      --create
      --if-not-exists
      --topic {{ item.name }}
      --partitions {{ item.partitions }}
      --replication-factor {{ item.replication_factor }}

      kafka-topics
      --zookeeper {{ groups['cp_zookeeper_nodes'][0] }}:2181
      --alter
      --topic {{ item.name }}
      {% for config in lookup('dict', item.config)  %}
      --config {{ config.key }}={{ config.value }}
      {% endfor %}
  loop: "{{ topics | flatten(levels=1) }}"
  when: kafka_topics_setup and (not absent_from_system)

- name: "Delete Kafka topics schema"
  uri:
    url: "http://{{ groups['cp_schema_registry'][0] }}:18081/subjects/{{ item.name }}-value"
    method: DELETE
  loop: "{{ topics | flatten(levels=1) }}"
  ignore_errors: true
  when: pristine_system or absent_from_system

- name: "Publish Kafka topics schema"
  uri:
    url: "http://{{ groups['cp_schema_registry'][0] }}:18081/subjects/{{ item.name }}-value/versions"
    headers:
      Content-Type: "application/vnd.schemaregistry.v1+json"
    method: POST
    body_format: json
    body: >-
      {"schema": "{{ lookup("file",  avro_schema_folder + "/" + item.avro.value_schema_file) |  regex_replace('\n| |\t', '') | regex_replace('"', '\"') }}"}
  loop: "{{ topics | flatten(levels=1) }}"
  when:  kafka_topics_setup and (not absent_from_system)

#############################################
########### Flink session & job #############

## TODO: clear Flink state in HDFS when: pristine_system or absent_from_system

## TODO: check how to auto restart when yarn recovers (nodes online)
## Source: https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/yarn_setup.html
## STOP FLINK's YARN SESSION:
## > echo "stop" | ./bin/yarn-session.sh -id "$(yarn application -list | grep flink-session-daemon | awk '{ print $1}')"
- name: "Flink detached session running"
  include: toolbox_exec.yml
  vars:
    toolbox_ignore_err: true
    toolbox_debug: true
    bash_one_liner: >-
      YARN_APPLICATION_FLINK_SESSION=$(yarn application -list | grep {{ flink_session_yarn_application_name }});
      [[ -z "${YARN_APPLICATION_FLINK_SESSION}" ]] &&
      yarn-session.sh
      -nm "{{ flink_session_yarn_application_name }}"
      -jm {{ flink.job_manager.memory }}
      -n  {{ flink.task_manager.node_count }}
      -s  {{ flink.task_manager.slots }}
      -tm {{ flink.task_manager.memory }}
      -st -d
      || true;
      YARN_APPLICATION_FLINK_SESSION=$(yarn application -list | grep {{ flink_session_yarn_application_name }});
      source /tmp/.yarn-properties-root;
      echo -e "\n\n\n";
      echo -e "Last Flink detached session YARN application id: ${applicationID}";
      echo -e "Flink admin UI: $(echo ${YARN_APPLICATION_FLINK_SESSION} | cut -d " " -f10)";
  when: flink_deploy_digestion_project

- name: "Build Flink job as artifact"
  delegate_to: 127.0.0.1
  shell: >-
    cd "{{ sensor_digestion_flink_path }}" &&
    sbt "set test in assembly := {}" clean assembly
  when: flink_build_digestion_project

- name: "Copy Flink job artifact to panda-toolbox"
  become: yes
  copy:
    src: "{{ sensor_digestion_flink_path }}/target/scala-2.11/sensor-digestion-flink-assembly-0.1-SNAPSHOT.jar"
    dest: "{{ toolbox.host.folders.state }}/sensor-digestion-flink-assembly.jar"
    owner: "{{ cluster_user }}"
    group: "{{ cluster_user }}"
    mode: "u=rx,g=rx"
  when: flink_build_digestion_project

- name: "Flink job running"
  include: toolbox_exec.yml
  vars:
    toolbox_ignore_err: true
    toolbox_debug: "{{ debug_mode }}"
    bash_one_liner: >-
      cd ${TOOLBOX_STATE};
      FLINK_JOB_ID=$(flink list -r | grep "{{ flink_job_name }} (RUNNING)" | cut -d " " -f4);
      [[ ! -z "${FLINK_JOB_ID}" ]] && echo "==> Cancel previous flink job {{ flink_job_name }}: ${FLINK_JOB_ID}" && flink cancel ${FLINK_JOB_ID};
      echo -e "\n==> Start flink job {{ flink_job_name }}";
      flink run -d sensor-digestion-flink-assembly.jar;
  when: flink_deploy_digestion_project

#############################################
########### Kafka connect DB sinks ##########

- name: "Copy Cassandra tables definition into toolbox"
  become: true
  template:
    src: "digestion_tables.cql.j2"
    dest: "{{ toolbox.host.folders.config }}/digestion_tables.cql"
    owner: "{{ cluster_user }}"
    group: "{{ cluster_user }}"
    mode: "u=rx,g=rx"

- name: "Cassandra tables definition up to date"
  include: toolbox_exec.yml
  vars:
    toolbox_ignore_err: false
    toolbox_debug: "{{ debug_mode }}"
    bash_one_liner: >-
      cqlsh -f {{ toolbox.container.exposed_folder.config }}/digestion_tables.cql

# Kafka Connect REST Interface: https://docs.confluent.io/current/connect/references/restapi.html
# Cassandra sink connector: https://docs.lenses.io/connectors/sink/cassandra.html
# To avoid slow re-balancing, consider: PAUSE all connectors, updates, RESUME all connectors
- name: "Kafka connectors config up to date"
  uri:
    url: "http://{{ groups['cp_kafka_connect_nodes'][0] }}:8083/connectors/{{ cassandra_sink.name }}/config/"
    headers:
      Accept: "application/json"
      Content-Type: "application/json"
    method: PUT
    body_format: json
    status_code: [200, 201]
    body: "{{ lookup('template', 'cassandra_connect_timeseries_sink.json.j2') }}"
  loop: "{{ kafka_connect_cassandra_sinks | flatten(levels=1) }}"
  loop_control:
    loop_var: cassandra_sink
